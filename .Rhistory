rownames(results_svm_df) <- "SVM"
# Wyświetl tabelę
results_svm_df
# Train – Heatmapa
autoplot(Train_CM_SVM, type = "heatmap") +
scale_fill_gradient(low = "white", high = "skyblue") +
ggtitle("Confusion Matrix – Train (SVM)") +
theme_minimal()
# Val – Heatmapa
autoplot(Val_CM_SVM, type = "heatmap") +
scale_fill_gradient(low = "white", high = "coral") +
ggtitle("Confusion Matrix – Validation (SVM)") +
theme_minimal()
# Tabela metryk
results_rf <- c(
Train_Acc_RF$.estimate,
Train_Prec_RF$.estimate,
Train_Rec_RF$.estimate,
Val_Acc_RF$.estimate,
Val_Prec_RF$.estimate,
Val_Rec_RF$.estimate
)
results_rf_df <- data.frame(t(round(results_rf, 2)))
colnames(results_rf_df) <- c(
"Train Accuracy",
"Train Precision ",
"Train Recall",
"Val Accuracy",
"Val Precision",
"Val Recall"
)
rownames(results_rf_df) <- "Random Forest"
# Wyświetl
results_rf_df
library(xgboost)
#Boosting
boost_model <- boost_tree(
trees = 100,
tree_depth = 4,
learn_rate = 0.05,
loss_reduction = 3,
sample_size = 0.75,
mtry = 3
) %>%
set_engine("xgboost") %>%
set_mode("classification")
boost_fit <- fit(boost_model,
Satisfaction ~ .,
data = Train_Oryginal)
# Train
Train_Preds_BOOST <- predict(boost_fit,
Train_Oryginal,
type = "prob") %>%
bind_cols(predict(boost_fit, Train_Oryginal)) %>%
bind_cols(Train_Oryginal %>% select(Satisfaction))
# Val
Val_Preds_BOOST <- predict(boost_fit,
Val_Oryginal,
type = "prob") %>%
bind_cols(predict(boost_fit, Val_Oryginal)) %>%
bind_cols(Val_Oryginal %>% select(Satisfaction))
# Train
Train_Acc_BOOST  <- accuracy(Train_Preds_BOOST, truth = Satisfaction, estimate = .pred_class)
Train_Prec_BOOST <- precision(Train_Preds_BOOST, truth = Satisfaction, estimate = .pred_class, event_level = "second")
Train_Rec_BOOST  <- recall(Train_Preds_BOOST, truth = Satisfaction, estimate = .pred_class, event_level = "second")
# Val
Val_Acc_BOOST  <- accuracy(Val_Preds_BOOST, truth = Satisfaction, estimate = .pred_class)
Val_Prec_BOOST <- precision(Val_Preds_BOOST, truth = Satisfaction, estimate = .pred_class, event_level = "second")
Val_Rec_BOOST  <- recall(Val_Preds_BOOST, truth = Satisfaction, estimate = .pred_class, event_level = "second")
results_boost <- c(
Train_Acc_BOOST$.estimate,
Train_Prec_BOOST$.estimate,
Train_Rec_BOOST$.estimate,
Val_Acc_BOOST$.estimate,
Val_Prec_BOOST$.estimate,
Val_Rec_BOOST$.estimate
)
results_boost_df <- data.frame(t(round(results_boost, 2)))
colnames(results_boost_df) <- c(
"Train Accuracy",
"Train Precision (Satisfied)",
"Train Recall (Satisfied)",
"Val Accuracy",
"Val Precision (Satisfied)",
"Val Recall (Satisfied)"
)
rownames(results_boost_df) <- "Boosting (XGBoost)"
# Wyświetlenie
results_boost_df
# Train
Train_CM_BOOST <- conf_mat(Train_Preds_BOOST, truth = Satisfaction, estimate = .pred_class)
# Val
Val_CM_BOOST <- conf_mat(Val_Preds_BOOST, truth = Satisfaction, estimate = .pred_class)
# Wizualizacja
autoplot(Train_CM_BOOST, type = "heatmap") +
scale_fill_gradient(low = "white", high = "green") +
ggtitle("Confusion Matrix – Train (Boosting)") +
theme_minimal()
autoplot(Val_CM_BOOST, type = "heatmap") +
scale_fill_gradient(low = "white", high = "red") +
ggtitle("Confusion Matrix – Validation (Boosting)") +
theme_minimal()
library(rio)
set.seed(2025)
dane <- import("airline_passenger_satisfaction.csv")
Dataset <- dane[sample(1:nrow(dane), 20000, replace = FALSE), ]
# Sprawdzanie braków danych
colSums(is.na(Dataset))
# Usunięcie niepotrzebnych kolumn ze zbioru danych (ID pożyczki) - unikalna wartość dla każdej instancji)
Dataset <- Dataset[-1]
# Zmiana nazw kolumn
library(dplyr)
Dataset <- Dataset %>%
rename(
Customer_Type = `Customer Type`,
Type_of_Travel = `Type of Travel`,
Flight_Distance = `Flight Distance`,
Departure_Delay = `Departure Delay`,
Arrival_Delay = `Arrival Delay`,
Departure_and_Arrival_Time_Convenience = `Departure and Arrival Time Convenience`,
Ease_of_Online_Booking = `Ease of Online Booking`,
Check_in_Service = `Check-in Service`,
Online_Boarding = `Online Boarding`,
Gate_Location = `Gate Location`,
On_board_Service = `On-board Service`,
Seat_Comfort = `Seat Comfort`,
Leg_Room_Service = `Leg Room Service`,
Food_and_Drink = `Food and Drink`,
In_flight_Service = `In-flight Service`,
In_flight_Wifi_Service = `In-flight Wifi Service`,
In_flight_Entertainment = `In-flight Entertainment`,
Baggage_Handling = `Baggage Handling`
)
# Zmiana typu danych z charakter na factor
Dataset$Gender <- as.factor(Dataset$Gender)
Dataset$Customer_Type <- as.factor(Dataset$Customer_Type)
Dataset$Type_of_Travel <- as.factor(Dataset$Type_of_Travel)
Dataset$Class <- as.factor(Dataset$Class)
Dataset$Satisfaction <- as.factor(Dataset$Satisfaction)
Dataset <- na.omit(Dataset)
colSums(is.na(Dataset_Imputation))
Dataset <- na.omit(Dataset)
colSums(is.na(Dataset))
library(dplyr)
library(ggplot2)
library(tidyverse)
long_data <- Dataset %>%
pivot_longer(cols = where(is.numeric))
# Utworzenie histogramów dla każdej zmiennej numerycznej z różnymi kolorami
ggplot(long_data, aes(x = value, fill = name)) +
geom_histogram(color = "black", bins = 30, show.legend = FALSE) +
facet_wrap(~ name, scales = "free") +
theme_minimal() +
labs(
title = "Histogramy dla każdej zmiennej numerycznej",
x = "Wartość",
y = "Częstotliwość"
) +
theme(
plot.title = element_text(hjust = 0.5)
)
library(rsample)
set.seed(2025)
# Tworzymy najpierw zbiór treningowy i testowy
split_initial <- initial_split(Dataset, prop = 0.8, strata = Satisfaction)
train <- training(split_initial)
test <- testing(split_initial)
# Dzielimy treningowy na trening i walidację ( 75/25 z tych 80%)
split_val <- initial_split(train, prop = 0.75, strata = Satisfaction)
train_set <- training(split_val)
val_set   <- testing(split_val)
library(themis)
Recipe <- recipe(Satisfaction ~ ., data = train_set) |>
step_log(c("Flight_Distance", "Departure_Delay", "Arrival_Delay"), offset = 1) |>     # transformacja logarytmiczna
step_dummy(all_nominal_predictors()) |>                                               # one-hot encoding
step_normalize(all_numeric_predictors()) |>                                           # normalizacja
step_smote(Satisfaction)                                                              # SMOTE – balansowanie klasy
prep_oryginal<- prep(Recipe)
Train_Oryginal <- bake(prep, new_data = NULL)
library(rsample)
library(tidyverse)
library(themis)
Recipe <- recipe(Satisfaction ~ ., data = train_set) |>
step_log(c("Flight_Distance", "Departure_Delay", "Arrival_Delay"), offset = 1) |>     # transformacja logarytmiczna
step_dummy(all_nominal_predictors()) |>                                               # one-hot encoding
step_normalize(all_numeric_predictors()) |>                                           # normalizacja
step_smote(Satisfaction)                                                              # SMOTE – balansowanie klasy
prep_oryginal<- prep(Recipe)
Train_Oryginal <- bake(prep, new_data = NULL)
library(themis)
library(recipes)
Recipe <- recipe(Satisfaction ~ ., data = train_set) |>
step_log(c("Flight_Distance", "Departure_Delay", "Arrival_Delay"), offset = 1) |>     # transformacja logarytmiczna
step_dummy(all_nominal_predictors()) |>                                               # one-hot encoding
step_normalize(all_numeric_predictors()) |>                                           # normalizacja
step_smote(Satisfaction)                                                              # SMOTE – balansowanie klasy
prep_oryginal<- prep(Recipe)
Train_Oryginal <- bake(prep, new_data = NULL)
library(themis)
library(recipes)
Recipe <- recipe(Satisfaction ~ ., data = train_set) |>
step_log(c("Flight_Distance", "Departure_Delay", "Arrival_Delay"), offset = 1) |>     # transformacja logarytmiczna
step_dummy(all_nominal_predictors()) |>                                               # one-hot encoding
step_normalize(all_numeric_predictors()) |>                                           # normalizacja
step_smote(Satisfaction)                                                              # SMOTE – balansowanie klasy
prep_oryginal<- prep(Recipe)
Train_Oryginal <- bake(prep_oryginal, new_data = NULL)
Val_Oryginal <- bake(prep_oryginal, new_data = val_set)
#------------------------------------------------------------
# 2. Logistic Regression
logistic_model <- logistic_reg() %>%
set_engine("glm") %>%
set_mode("classification")
logistic_wf <- workflow() %>%
add_model(logistic_model) %>%
add_recipe(Recipe)
logistic_fit <- fit(logistic_wf, data = Train_Oryginal)
# Tworzymy najpierw zbiór treningowy i testowy
split_initial <- initial_split(Dataset, prop = 0.8, strata = Satisfaction)
set.seed(2025)
# Tworzymy najpierw zbiór treningowy i testowy
split_initial <- initial_split(Dataset, prop = 0.8, strata = Satisfaction)
train <- training(split_initial)
test <- testing(split_initial)
# Dzielimy treningowy na trening i walidację ( 75/25 z tych 80%)
split_val <- initial_split(train, prop = 0.75, strata = Satisfaction)
train_set <- training(split_val)
val_set   <- testing(split_val)
# 1. Recipe (wspolny dla wszystkich modeli)
recipe <- recipe(Satisfaction ~ ., data = train_set) |>
step_log(c("Flight_Distance", "Departure_Delay", "Arrival_Delay"), offset = 1) |>     # transformacja logarytmiczna
step_dummy(all_nominal_predictors()) |>                                               # one-hot encoding
step_normalize(all_numeric_predictors()) |>                                           # normalizacja
step_smote(Satisfaction)                                                              # SMOTE – balansowanie klasy
#------------------------------------------------------------
# 2. Logistic Regression
logistic_model <- logistic_reg() %>%
set_engine("glm") %>%
set_mode("classification")
logistic_wf <- workflow() %>%
add_model(logistic_model) %>%
add_recipe(recipe)
logistic_fit <- fit(logistic_wf, data = train_set)
#------------------------------------------------------------
# 3. Decision Tree
tree_model <- decision_tree() %>%
set_engine("rpart") %>%
set_mode("classification")
tree_wf <- workflow() %>%
add_model(tree_model) %>%
add_recipe(recipe)
tree_fit <- fit(tree_wf, data = train_set)
#------------------------------------------------------------
# 4. Random Forest
rf_model <- rand_forest() %>%
set_engine("ranger") %>%
set_mode("classification")
rf_wf <- workflow() %>%
add_model(rf_model) %>%
add_recipe(Recipe)
rf_fit <- fit(rf_wf, data = train_set)
svm_wf <- workflow() %>%
add_model(svm_model) %>%
add_recipe(Recipe)
svm_wf <- workflow() %>%
add_model(svm_model) %>%
add_recipe(recipe)
#------------------------------------------------------------
# 5. SVM
svm_model <- svm_rbf() %>%
set_engine("kernlab") %>%
set_mode("classification")
svm_wf <- workflow() %>%
add_model(svm_model) %>%
add_recipe(recipe)
svm_fit <- fit(svm_wf, data = train_set)
svm_fit <- fit(svm_wf, data = train_set)
#------------------------------------------------------------
# 6. Boosting
boost_model <- boost_tree(
trees = 100,
tree_depth = 4,
learn_rate = 0.05,
loss_reduction = 3,
sample_size = 0.75,
mtry = 3
) %>%
set_engine("xgboost") %>%
set_mode("classification")
boost_wf <- workflow() %>%
add_model(boost_model) %>%
add_recipe(recipe)
boost_fit <- fit(boost_wf, data = train_set)
#------------------------------------------------------------
# Ewaluacja funkcji pomocniczej
evaluate_model <- function(model_fit, val_data, truth_col = Satisfaction) {
preds <- predict(model_fit, new_data = val_data, type = "prob") %>%
bind_cols(predict(model_fit, new_data = val_data)) %>%
bind_cols(val_data %>% select(!!truth_col))
metrics_vals <- metrics(preds, truth = !!truth_col, estimate = .pred_class)
cm <- conf_mat(preds, truth = !!truth_col, estimate = .pred_class)
list(metrics = metrics_vals, confusion_matrix = cm)
}
#------------------------------------------------------------
# Ewaluacja wszystkich modeli
results_logistic <- evaluate_model(logistic_fit, val_set)
#------------------------------------------------------------
evaluate_model <- function(model_fit, val_data, truth_col) {
truth_col <- rlang::enquo(truth_col)
preds <- predict(model_fit, new_data = val_data, type = "prob") %>%
bind_cols(predict(model_fit, new_data = val_data)) %>%
bind_cols(val_data %>% dplyr::select(!!truth_col))
metrics_vals <- metrics(preds, truth = !!truth_col, estimate = .pred_class)
cm <- conf_mat(preds, truth = !!truth_col, estimate = .pred_class)
list(metrics = metrics_vals, confusion_matrix = cm)
}
#------------------------------------------------------------
# Ewaluacja wszystkich modeli
results_logistic <- evaluate_model(logistic_fit, val_set, truth_col = Satisfaction)
results_tree     <- evaluate_model(tree_fit, val_set, truth_col = Satisfaction)
results_rf       <- evaluate_model(rf_fit, val_set, truth_col = Satisfaction)
results_svm      <- evaluate_model(svm_fit, val_set, truth_col = Satisfaction)
results_boost    <- evaluate_model(boost_fit, val_set, truth_col = Satisfaction)
View(results_rf)
#------------------------------------------------------------
# Kolorowy confusion matrix (przyklad dla Random Forest)
autoplot(results_rf$confusion_matrix, type = "heatmap") +
scale_fill_gradient(low = "white", high = "red") +
labs(title = "Confusion Matrix - Random Forest (Val)")
results_logistic$metrics
#------------------------------------------------------------
evaluate_model <- function(model_fit, val_data, truth_col) {
truth_col <- rlang::enquo(truth_col)
preds <- predict(model_fit, new_data = val_data, type = "prob") %>%
bind_cols(predict(model_fit, new_data = val_data)) %>%
bind_cols(val_data %>% dplyr::select(!!truth_col))
# Definicja zestawu metryk
metric_set_all <- metric_set(accuracy, precision, recall, f_meas)
metrics_vals <- metric_set_all(preds, truth = !!truth_col, estimate = .pred_class)
cm <- conf_mat(preds, truth = !!truth_col, estimate = .pred_class)
list(metrics = metrics_vals, confusion_matrix = cm)
}
#------------------------------------------------------------
# Ewaluacja wszystkich modeli
results_logistic <- evaluate_model(logistic_fit, val_set, truth_col = Satisfaction)
results_tree     <- evaluate_model(tree_fit, val_set, truth_col = Satisfaction)
results_rf       <- evaluate_model(rf_fit, val_set, truth_col = Satisfaction)
results_rf       <- evaluate_model(rf_fit, val_set, truth_col = Satisfaction)
results_svm      <- evaluate_model(svm_fit, val_set, truth_col = Satisfaction)
results_boost    <- evaluate_model(boost_fit, val_set, truth_col = Satisfaction)
results_logistic$metrics
results_tree$metrics
results_rf$metrics
results_svm$metrics
results_boost$metrics
#------------------------------------------------------------
evaluate_model <- function(model_fit, val_data, truth_col) {
truth_col <- rlang::enquo(truth_col)
preds <- predict(model_fit, new_data = val_data, type = "prob") %>%
bind_cols(predict(model_fit, new_data = val_data)) %>%
bind_cols(val_data %>% dplyr::select(!!truth_col))
# Definicja zestawu metryk
metric_set_all <- metric_set(accuracy, precision, recall, f1-score)
metrics_vals <- metric_set_all(preds, truth = !!truth_col, estimate = .pred_class)
cm <- conf_mat(preds, truth = !!truth_col, estimate = .pred_class)
list(metrics = metrics_vals, confusion_matrix = cm)
}
#------------------------------------------------------------
# Ewaluacja wszystkich modeli
results_logistic <- evaluate_model(logistic_fit, val_set, truth_col = Satisfaction)
results_tree     <- evaluate_model(tree_fit, val_set, truth_col = Satisfaction)
#------------------------------------------------------------
# Kolorowy confusion matrix (przyklad dla Random Forest)
autoplot(results_rf$confusion_matrix, type = "heatmap") +
scale_fill_gradient(low = "white", high = "red") +
labs(title = "Confusion Matrix - Random Forest (Val)")
# Analogicznie mozesz uzyc:
# autoplot(results_logistic$confusion_matrix, type = "heatmap") itd.
# Logistic Regression
autoplot(results_logistic$confusion_matrix, type = "heatmap") +
ggtitle("Confusion Matrix - Logistic Regression") +
scale_fill_gradient(low = "white", high = "steelblue") +
theme_minimal()
# Decision Tree
autoplot(results_tree$confusion_matrix, type = "heatmap") +
ggtitle("Confusion Matrix - Decision Tree") +
scale_fill_gradient(low = "white", high = "steelblue") +
theme_minimal()
results_tree$metrics
results_rf$metrics
# Kolorowy confusion matrix
autoplot(results_rf$confusion_matrix, type = "heatmap") +
scale_fill_gradient(low = "white", high = "red") +
labs(title = "Confusion Matrix - Random Forest (Val)")
results_svm$metrics
# Boosting
autoplot(results_svm$confusion_matrix, type = "heatmap") +
ggtitle("Confusion Matrix - Boosting") +
scale_fill_gradient(low = "white", high = "steelblue") +
theme_minimal()
results_boost$metrics
# Boosting
autoplot(results_boost$confusion_matrix, type = "heatmap") +
ggtitle("Confusion Matrix - Boosting") +
scale_fill_gradient(low = "white", high = "steelblue") +
theme_minimal()
results_logistic <- evaluate_model(logistic_fit, train_set, truth_col = Satisfaction)
evaluate_model <- function(model_fit, val_data, truth_col) {
truth_col <- rlang::enquo(truth_col)
preds <- predict(model_fit, new_data = val_data, type = "prob") %>%
bind_cols(predict(model_fit, new_data = val_data)) %>%
bind_cols(val_data %>% dplyr::select(!!truth_col))
# Definicja zestawu metryk
metric_set_all <- metric_set(accuracy, precision, recall, f_meas)
metrics_vals <- metric_set_all(preds, truth = !!truth_col, estimate = .pred_class)
cm <- conf_mat(preds, truth = !!truth_col, estimate = .pred_class)
list(metrics = metrics_vals, confusion_matrix = cm)
}
# Logistic Regression
logistic_model <- logistic_reg() %>%
set_engine("glm") %>%
set_mode("classification")
logistic_wf <- workflow() %>%
add_model(logistic_model) %>%
add_recipe(recipe)
logistic_fit <- fit(logistic_wf, data = train_set)
results_logistic <- evaluate_model(logistic_fit, train_set, truth_col = Satisfaction)
results_logistic$metrics
results_logistic_train <- evaluate_model(logistic_fit, train_set, truth_col = Satisfaction)
results_logistic_train$metrics
results_logistic <- evaluate_model(logistic_fit, val_set, truth_col = Satisfaction)
results_logistic$metrics
# Logistic Regression
autoplot(results_logistic_train$confusion_matrix, type = "heatmap") +
ggtitle("Confusion Matrix - Logistic Regression") +
scale_fill_gradient(low = "white", high = "steelblue") +
theme_minimal()
autoplot(results_logistic$confusion_matrix, type = "heatmap") +
ggtitle("Confusion Matrix - Logistic Regression") +
scale_fill_gradient(low = "white", high = "steelblue") +
theme_minimal()
# Logistic Regression
autoplot(results_logistic_train$confusion_matrix, type = "heatmap") +
ggtitle("Confusion Matrix - Logistic Regression") +
scale_fill_gradient(low = "white", high = "steelblue") +
theme_minimal()
autoplot(results_logistic$confusion_matrix, type = "heatmap") +
ggtitle("Confusion Matrix - Logistic Regression") +
scale_fill_gradient(low = "white", high = "steelblue") +
theme_minimal()
results_tree_train <- evaluate_model(tree_fit, val_set, truth_col = Satisfaction)
results_tree_train <- evaluate_model(tree_fit, train_set, truth_col = Satisfaction)
results_tree <- evaluate_model(tree_fit, val_set, truth_col = Satisfaction)
results_tree_train$metrics
results_tree$metrics
results_tree_train$metrics
results_tree$metrics
results_logistic_train$metrics
results_logistic$metrics
# Decision Tree
autoplot(results_tree_train$confusion_matrix, type = "heatmap") +
ggtitle("Confusion Matrix - Decision Tree") +
scale_fill_gradient(low = "white", high = "steelblue") +
theme_minimal()
autoplot(results_tree$confusion_matrix, type = "heatmap") +
ggtitle("Confusion Matrix - Decision Tree") +
scale_fill_gradient(low = "white", high = "steelblue") +
theme_minimal()
results_rf_train <- evaluate_model(rf_fit, train_set, truth_col = Satisfaction)
results_rf <- evaluate_model(rf_fit, val_set, truth_col = Satisfaction)
results_rf_train <- evaluate_model(rf_fit, train_set, truth_col = Satisfaction)
results_rf <- evaluate_model(rf_fit, val_set, truth_col = Satisfaction)
results_rf_train <- evaluate_model(rf_fit, train_set, truth_col = Satisfaction)
results_rf <- evaluate_model(rf_fit, val_set, truth_col = Satisfaction)
results_rf_train$metrics
results_rf$metrics
# Kolorowy confusion matrix
autoplot(results_rf_train$confusion_matrix, type = "heatmap") +
scale_fill_gradient(low = "white", high = "red") +
labs(title = "Confusion Matrix - Random Forest (Val)")
autoplot(results_rf$confusion_matrix, type = "heatmap") +
scale_fill_gradient(low = "white", high = "red") +
labs(title = "Confusion Matrix - Random Forest (Val)")
results_svm <- evaluate_model(svm_fit, train_set, truth_col = Satisfaction)
results_svm <- evaluate_model(svm_fit, val_set, truth_col = Satisfaction)
results_svm_train$metrics
results_svm_train <- evaluate_model(svm_fit, train_set, truth_col = Satisfaction)
results_svm <- evaluate_model(svm_fit, val_set, truth_col = Satisfaction)
results_svm_train$metrics
results_svm$metrics
# SVM
autoplot(results_svm_train$confusion_matrix, type = "heatmap") +
ggtitle("Confusion Matrix - Boosting") +
scale_fill_gradient(low = "white", high = "steelblue") +
theme_minimal()
autoplot(results_svm$confusion_matrix, type = "heatmap") +
ggtitle("Confusion Matrix - Boosting") +
scale_fill_gradient(low = "white", high = "steelblue") +
theme_minimal()
results_boost_train<- evaluate_model(boost_fit, train_set, truth_col = Satisfaction)
results_boost<- evaluate_model(boost_fit, val_set, truth_col = Satisfaction)
results_boost_train$metrics
results_boost$metrics
# Boosting
autoplot(results_boost_train$confusion_matrix, type = "heatmap") +
ggtitle("Confusion Matrix - Boosting") +
scale_fill_gradient(low = "white", high = "steelblue") +
theme_minimal()
autoplot(results_boost$confusion_matrix, type = "heatmap") +
ggtitle("Confusion Matrix - Boosting") +
scale_fill_gradient(low = "white", high = "steelblue") +
theme_minimal()
